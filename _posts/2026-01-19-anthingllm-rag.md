---
layout: post
title: AnythingLLM
date: 2026-01-19
reading_time: 20 min read
tags: [AI, RAG]
excerpt: 
update: 2026-02-12
---

這份指南是使用 **AnythingLLM** 打造一個既能檢索文件、又能處理圖片的 **RAG（檢索增強生成）** 系統。

---

## 💡 什麼是 AnythingLLM？

AnythingLLM 是一款全能型的私有知識庫解決方案，它將大語言模型 (LLM)、向量數據庫 (Vector DB) 和文檔處理工具整合在一起，讓你能在不外流數據的情況下，與自己的資料進行對話。

---

## 第一部分：基礎 RAG 搭建流程（純文字與文件）

### 1. 配置核心三要素

在啟動 AnythingLLM 後，需先設定以下引擎：

* **LLM (大語言模型)：** 負責理解與回答。可選擇內置模型，或連結 **Ollama** (建議 Llama 3 或 Qwen 2)。
* **Embedding Model (向量化模型)：** 負責將文檔轉化為 AI 能理解的數學向量。建議使用內置的 **AnythingLLM Embedder**。
* **Vector Database (向量數據庫)：** 儲存文檔片段。默認使用 **LanceDB**，效能極佳。

### 2. 建立與管理工作區 (Workspace)

* AnythingLLM 以「工作區」隔離不同主題（如：合約、技術手冊、個人筆記）。
* 在特定工作區中上傳文件，可避免 AI 在回答時抓取到無關的干擾資訊。

### 3. 文檔處理與嵌入

* **上傳：** 支援 PDF, TXT, Docx, Markdown 甚至網頁 URL。
* **移動與保存：** 選取文件並點擊 **"Save and Embed"**。系統會自動將文檔「切片」並存入數據庫。

---

## 第二部分：進階功能——融合圖片處理 (Multimodal RAG)

若要讓 RAG 系統具備「看圖說話」的能力，可分為以下兩個場景：

### 1. 聊天時的圖片分析（多模態對話）

如果你希望在對話中臨時上傳圖片讓 AI 分析：

* **關鍵條件：** 必須選用 **Vision-capable (具備視覺能力)** 的模型，如雲端的 `GPT-4o` 或本地的 `Llava` (透過 Ollama)。
* **應用場景：** 上傳一張報警截圖，問 AI：「根據我知識庫裡的排錯手冊，這張圖代表什麼問題？」

### 2. 知識庫內的圖片檢索（圖片 RAG）

處理儲存在知識庫中的圖表或圖片：

* **內置 OCR 功能：** 上傳 PDF 時，系統會自動辨識圖中的文字並轉為索引，適合處理掃描件。
* **純圖片索引（進階建議）：** 如果是大量純圖片，建議先用多模態模型為圖片生成「文字描述檔」，再將描述檔匯入 AnythingLLM，以達成更精準的檢索。

---

## 第三部分：配置建議表

| 組件類型 | 推薦選擇 | 說明 |
| --- | --- | --- |
| **多模態 LLM** | `Llava` (本地) / `GPT-4o` (雲端) | 必備視覺能力，才能「看懂」圖片內容 |
| **本地運行環境** | `Ollama` | 搭配 AnythingLLM 的最佳本地模型管理器 |
| **對話模式** | `Query Mode` | 嚴格限制 AI 僅根據你的文件內容回答，防止胡謅 |
| **硬體需求** | 8GB+ 顯存 (VRAM) | 若要流暢運行具備視覺能力的本地模型，顯存至關重要 |

---

## 🏆 實戰調優技巧

* **引用溯源 (Citations)：** 每次回答後點擊引用，確認 AI 是參考了哪一頁、哪張圖，確保正確性。
* **相似度閾值 (Similarity Threshold)：** 若覺得 AI 找得不夠準，可調高此數值，過濾掉關聯性低的資料。

---

# AnythingLLM 加速 

AnythingLLM（本質上是 RAG：Retrieval-Augmented Generation 架構）**查詢速度慢通常不是「模型推理」本身，而是「向量搜尋 + 檢索流程」造成的**。可以從以下幾個面向優化：

---

## 🔎 一、優化向量資料庫（最有效）

AnythingLLM 常搭配：

* Chroma
* LanceDB
* Qdrant
* Weaviate
* Milvus

### ✅ 1️⃣ 換成高效能向量資料庫

如果你現在用：

* **Chroma（本地 SQLite）** → 可考慮改成 **Qdrant 或 Milvus**
* 本地單機 → 可考慮改成獨立向量 DB server

👉 Qdrant 在大資料量下通常會更快

---

### ✅ 2️⃣ 啟用 HNSW 索引（如果支援）

高效能向量 DB 都支援 HNSW（近似最近鄰搜尋）：

優化參數：

* `ef_search`（越高越準確但越慢）
* `M`（索引圖的連接數）

📌 建議：

* 把 ef_search 調低（例如 64 → 32）
* 如果資料很多，可稍微降低準確度換速度

---

### ✅ 3️⃣ 減少向量維度

如果是用：

* 1536 維（OpenAI embedding）
* 3072 維

可以改成：

* 384 維（例如 `all-MiniLM-L6-v2`）
* 768 維

👉 維度越小，搜尋越快，記憶體也更省

---

## 📦 二、優化文件切分（Chunking）

很多人忽略這點，其實影響很大。

### ❌ 不好的做法

* chunk 太小（例如 100 tokens）
* chunk 太多（幾十萬筆）

### ✅ 建議

* 500–1000 tokens / chunk
* overlap 100–150

👉 減少 chunk 數量 = 減少搜尋筆數 = 速度提升

---

## ⚡ 三、降低 Top-K

如果現在設定為：

```
Top K = 10
```

試著改成：

```
Top K = 3 或 5
```

👉 搜尋 + rerank 都會變快

很多情況 K=3 已經足夠。

---

## 🧠 四、關閉或優化 Reranker

如果有啟用：

* Cross-encoder reranker
* ColBERT

👉 這會很慢

可以：

* 關掉 reranker
* 或降低 rerank 數量

---

## 🖥 五、模型推理優化（如果是本地 LLM）

如果用 Ollama / LM Studio：

### ✅ 1️⃣ 使用較小模型

* 13B → 改 7B
* 7B → 改 4B

### ✅ 2️⃣ 使用量化版本

* Q4_K_M
* Q5_K_M

### ✅ 3️⃣ GPU 推理

* CPU → 慢很多
* GPU → 快 5~10 倍

---

## 🚀 六、開啟快取（Cache）

AnythingLLM 支援：

* Embedding cache
* Response cache

👉 常問問題會直接回傳，不會重新查詢

---

## 📊 七、資料量級別對應優化建議

| 文件量            | 建議做法            |
| -------------- | --------------- |
| < 5,000 chunks | 基本不用優化          |
| 5,000–50,000   | 調整 chunk + 降 K  |
| 50,000–500,000 | 換 Qdrant + HNSW |
| > 500,000      | 專用向量 DB + 分區索引  |

---

## 🎯 實戰最快優化順序（推薦）

如果覺得慢，請照這順序試：

1. 把 Top-K 改成 3
2. 關 reranker
3. chunk size 改 800
4. 改用 Qdrant
5. embedding 換 384 維模型

通常能快 2～10 倍。


